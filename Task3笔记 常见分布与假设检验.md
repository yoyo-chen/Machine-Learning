参考资料
[DataWhale 组队学习/数据挖掘基础方法/概率统计/3. 常见分布与假设检验](https://github.com/datawhalechina/team-learning/blob/master/02%20%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80%E6%96%B9%E6%B3%95/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/3.%20%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E4%B8%8E%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C.md)

[概率论与数理统计-陈希孺](https://github.com/yoyo-chen/Machine-Learning/blob/master/概率论与数理统计%20-%20陈希孺.pdf) 



#### 1. 一般随机变量

##### 1.1 离散型随机变量

* 概率质量函数 (probability mass function) PMF

* 例：二项分布，泊松分布

##### 1.2 连续型随机变量

* 概率密度函数 (probability density function) PDF

* 累积分布函数 (cumulative distribution function) CDF 

* 例：均匀分布，正态分布，指数分布



#### 2. 常见分布

##### 2.1 离散型分布

* 二项分布  $B(n, p)$

  在n次试验中，单次试验成功率为$p$，失败率$q=1-p$，则成功$i$次的概率为，
  $$
  p_i=b(i;n,p)=C_n^ip^i(1-p)^{n-i}, i=0, 1, ..., n
  $$
  
* 泊松分布  $P(\lambda)$

  【例】一定时间内某交通路口所发生的事故个数。

  设所观察的这段时间为 [0,1)，取一个很大的自然数n，把时间段[0,1)分为等长的$n$段。

   假设：

  1. 每段时间足够短，只可能发生一个事故，且发生一个事故的概率为$\frac{\lambda}{n}$。
  2. 各段时间内是否发生事故是独立的。

  事故数$X$可视为在$n$个小段内有事故的时间段个数，服从二项分布$B(n,\frac{\lambda}{n})$ ，于是
  $$
  P(X=i)=C_n^i(\frac{\lambda}{n})^i(1-\frac{\lambda}{n})^{n-1}
  $$
  当$n$取极限时，就得到确切的答案。当$n\rightarrow\infty$时，
  
  $$
  \frac{C_n^i}{n^i}\rightarrow \frac{1}{i!}
  $$

  $$
  (\frac{\lambda}{n})^n\rightarrow e^{-\lambda}
  $$

  由此得出
  $$
  P(X=i)=e^{-\lambda} \frac{\lambda^i}{i!} 
  $$

  泊松分布的另一个例子[每天卖多少只馒头](https://blog.csdn.net/ccnt_2012/article/details/81114920)



* 二项分布，泊松分布，正态分布的关系

   若 $X \sim B(n, p)$，其中$n$很大，$p$很小而$np=\lambda$不太大时，则$X$的分布接近于泊松分布。

  $\rightarrow$ 可以将较难计算的二项分布转化为泊松分布计算。

  

* 其他离散型分布

  a. 几何分布

  考虑独立重复试验，几何分布描述的是经过k次试验才首次获得成功的概率，假定每次成功率为p，
  $$
  P\lbrace X= k \rbrace = {(1-p)}^{k-1} p
  $$

  b. 负二项分布

  考虑独立重复试验，负二项分布描述的是试验一直进行到成功r次的概率，假定每次成功率为p，
  $$
  P\lbrace X= n \rbrace = C_{n-1}^{r-1} p^r {(1-p)}^{n-r}
  $$
  
  c. 超几何分布
  
  超几何分布描述的是在一个总数为N的总体中进行有放回地抽样，其中在总体中k个元素属于一组，剩余N-k个元素属于另一组，假定从总体中抽取n次，其中包含x个第一组的概率为
$$
P\lbrace X= n \rbrace = \frac {C_{k}^{x} C_{N-k}^{n-x}} {C_{N}^{n}}
$$



##### 2.2 连续型分布

* 均匀分布  $R(a, b)$
  $$
  f(x)=\begin{cases}\frac {1} {b-a} ,  &  a \leq x  \leq b \\0, & others\end{cases}
  $$

* 正态分布 $N(\mu, \sigma^2)$
  $$
  f(x)=\frac{1}{\sqrt{2π}\sigma}e^{\frac{-(x-u)^2}{2\sigma^2}}
  $$

* **指数分布**  $E(\lambda)$
  $$
  f(x)=\begin{cases}λe^{-λx} ,  &   x  > 0 \\0, & x \leq 0\end{cases}
  $$
  
  $$
  F(x)=\begin{cases}1-e^{-λx} ,  &   x  > 0 \\0, & x \leq 0\end{cases}
  $$
  
  指数分布看起来很有趣！
  
  【例】寿命分布
  
  设一种大批生产的电子元件的寿命$X$是随机变量，以$F$记$X$的分布函数。
  
  假设元件不发生老化，就是说，元件在时刻$x$尚能正常工作的条件下，其失效率总为常数$\lambda$。失效率是指单位长度时间内失效的概率（瞬时失效率）。上述假设如下式所示，
  $$
  P(x \leq X \leq x+h | X >x )/h = \lambda, h \rightarrow 0
  $$
  此式解释如下：元件在时刻$x$时能够正常工作，表示其寿命大于$x$，即$X>x$。在$x$处，长为$h$的时间段内失效，即$x\leq X \leq x+h$。把这个条件概率除以时间段之长$h$，即得在$x$时刻得平均失效率。再令$h\rightarrow0$，得瞬时失效率，按假定，它应为常数$\lambda$。
  
  根据定义可知，
  $$
  P(X>x) = 1-F(x)
  $$
  
  $$
  \{X>x\}\{x\leq X \leq x+h\}=\{x<X\leq x+h\}
  $$
  
  因此，
  $$
  P(x\leq X\leq x+h | X>x)/h
  $$
  
  $$
  =P(x<X\leq x+h)/(h(1-F(x)))
  $$
  
  $$
  =[(F(x+h)-F(x))/h]/(1-F(x))
  $$
  
  $$
  \rightarrow F'(x)/(1-F(x))=\lambda
  $$
  
  这个微分方程的通解为$F(x)=1-Ce^{\lambda x}$（当$x>0$。$x\leq0$时$F(x)$为0）。常数$C$可用初始条件$F(0)=0$（因为$F(0)=P(X\leq 0)$，而寿命$\leq 0$的概率为0）定出为1，这样就得到上述的分布函数。
  
  
  
  梳理一下指数分布中的各个概率：元件的瞬时失效率是常数$\lambda$，从当前时间点经过时间$x$失效的概率是$f(x)$，寿命短于$x$的概率是$F(x)$。
  
  
  
  关于指数分布还有一个有趣的性质的是指数分布是无记忆性的，假定在等候事件发生的过程中已经过了一些时间，此时距离下一次事件发生的时间间隔的分布情况和最开始是完全一样的，就好像中间等候的那一段时间完全没有发生一样，也不会对结果有任何影响（因为没有考虑“老化”，也就是说，该事件在任意一个瞬时发生的概率是一个常数），用数学语言来表述是
  $$
  P\{X>s+t | X> t\} =P\{X>s\}
  $$
  指数分布的另一个例子：[卖出馒头的时间间隔](https://blog.csdn.net/ccnt_2012/article/details/89875865)
  
  
  
* 其他连续型分布

  a. $\Gamma$分布

  常用来描述某个事件发生n次的等待时间的分布。

  

  **b. 威布尔分布**

  指数分布的例子里没有考虑元件的老化，威布尔分布则可以考虑老化的影响。此处的失效率不是常数，而是一个$x$的增函数，$\lambda x^m (\lambda>0, m>0)$。根据和指数分布相同的推导过程可得
  $$
  F'(x)/(1-F(x))=\lambda x^m
  $$
  结合初始条件$F(0)=0$，得出，
  $$
  F(x)=1-e^{(-\lambda/m+1) x^{m+1}}
  $$
  取$a=m+1(a>1)$，并把$\lambda/(m+1)$记为$\lambda$，得出
  $$
  F(x)=1-e^{(-\lambda) x^a}
  $$
  威布尔分布和指数分布在可靠性统计分析中占重要地位。

  

##### 2.3 常见分布的均值和方差汇总 （见DataWhale参考资料）

##### 2.4 Python代码实战 （见DataWhale参考资料）

* 生成一组符合特定分布的随机数

* 计算统计分布的PMF和PDF

* 计算统计分布的CDF

  以上三条都很easy，一行代码就可以搞定。

* 统计分布可视化

  这个用的是sns.distplot()

  【例】二项分布

  ```python
  t = stats.binom.rvs(10,0.5,size=10000) # B(10,0.5)随机抽样10000次
  
  sns.distplot(t,bins=10,hist_kws={'density':True}, kde=False,label = 'Distplot from 10000 samples')
  ```



#### 3. 假设检验

##### 3.1 问题提法和基本概念

<font color = blue>注：3.1节中的各种名词很可能给你一种莫名其妙的感觉。先认识一下就好，3.2节将结合具体的例子给出直观的解释。</font>



###### 3.1.1 问题提法

还是2.2节中元件寿命服从指数分布的例子，现在的问题是，通过对抽出的若干个元件进行测试所得的数据，来判断“元件平均寿命不小于5000小时”是否成立。用统计学的语言表述如下：



1. 有一个总体，即一批元件的寿命，我们假设该总体的分布服从指数分布，该分布包含了一个未知参数$\lambda$

2. 从该总体中抽出样本$X_1,\cdots,X_n$，即抽出的那$n$个元件测试出的寿命。

3. 现在有一个需要**判断**的**命题**：“元件平均寿命不小于5000小时”，即“$\frac{1}{\lambda}\geq5000$”。它把参数$\lambda$所有可能取得值$0<\lambda<\infty$分成两部分：$H_0=\{\lambda: \lambda \leq 1/5000\}$，$H_1=\{\lambda: \lambda > 1/5000\}$。$H_0$内的$\lambda$使上述命题成立，$H_1$内的$\lambda$使上述命题不成立。因此该命题可记为：“$\lambda$属于$H_0$”，或写为“$\lambda \in H_0$”，或只写为$H_0$。

4. 任务是利用所获得的样本，去判断命题是否成立。

   

- 在数理统计学上，将上述**命题**称为**假设**，**判断**称为**检验**。注意检验兼具动词和名词属性，动词指判断全过程的操作，名词指判断准则，通常记为$\Phi$。
- $H_0$：原假设，$H_1$：对立假设（全体：$\lambda > 1/5000$，或特殊情况：$\lambda = 2/5000$）。
- 检验统计量$\overline X$：检验一个假设时所使用的统计量，例子中为元件寿命的均值。
- 接受域$A$：使原假设被接受的样本所在的区域。
- 否定域（拒绝域、临界域）$R$：使原假设被否定的样本所在的区域。
- 临界值：划分接受域与否定域的统计量的值。
- 简单假设：单参数。复合假设：多参数。



###### 3.1.2 功效函数 $\beta_\Phi$

- 原假设被否定的概率



###### 3.1.3 两类错误、检验的水平

- 两类错误：

  1. $H_0$正确，但被否定了

  2. $H_0$不正确，但被接受了

  为什么要区分这两个看起来相同的错误？后文将结合具体例子解释。

- 检验的水平：

  选择一个检验时，要使其功效函数在$H_0$上尽量小而在$H_1$上尽量大。

  但这两方面的要求是矛盾的（怎么矛盾了也会在后文解释）。可以通过“**保一望二**”原则解决这个问题：

  先保证第一类错误的概率不超过某指定值$\alpha$（通常较小，最常用的是$\alpha=0.05$和$0.01$，有时也用到$0.001，0.1，$以至$0.2$等值），再在这限制下，使第二类错误概率尽可能小。
  $$
  \beta_\Phi(\theta_1,\cdots,\theta_k)\leq \alpha, 对任何(\theta_1,\cdots,\theta_k)\in H_0
  $$
  
  
  $\alpha$为常数，$0\leq \alpha \leq 1$。称$\Phi$为$H_0$的一个水平$\alpha$的检验，或者说，检验$\Phi$的水平为$\alpha$，检验$\Phi$有水平$\alpha$。
  
  只要可能，尽量找最小的$\alpha$。

  ```tex
注：以上所述“固定（或限制）第一类错误概率的原则”是一种目前被广泛采用的做法。在多数假设检验问题中，第一类错误被认为更有害，更需要控制。但也并非绝对，有时第二类错误的危害更大，因此“固定（或限制）第一类错误概率的原则”也并非绝对，可根据具体情况的需要变通。
  ```



###### 3.1.4 一致最优检验

若对任意其他一个水平$\alpha$的检验$g$，必有
$$
\beta_\Phi(\theta_1,\cdots,\theta_k)\geq \beta_g(\theta_1,\cdots,\theta_k), 对任何(\theta_1,\cdots,\theta_k)\in H_1
$$
则称$\Phi$是假设检验问题$H_0:H_1$的一个水平$\alpha$的一致最优检验。

```latex
注1：上述定义换句话说，就是在一切其第一类错误概率不超过$\alpha$的检测中，第二类错误的概率处处达到最小者（功效函数在队里假设上处处达到最大）。
注2：一致最优检验要求“处处”成立，很难满足，一般对总体分布有一定限制时才存在。
```



##### 3.2 重要参数检验

###### 3.2.1 正态总体均值的检验

<font color = blue>注：这个例子在描述过程中将对3.1节中的大部分名词给出直观解释，虽然有点长，但分析过程非常精彩。吃透这个例子，参数检验这一块就算是开悟了。</font>

设$X_1,\cdots,X_n$是自正态总体$N(\theta,\sigma^2)$中抽出的样本，下面来讨论有关均值$\theta$的假设检验问题。在应用上常见的形式有以下三种，这里$\theta_0$是一个给定的数。

　　①　$H_0: \theta \geq \theta_0, H_1: \theta < \theta_0$

　　②　$H_0': \theta \leq \theta_0, H_1': \theta > \theta_0$

　　③　$H_0'': \theta = \theta_0, H_1'': \theta \neq \theta_0$

方差已知还是未知将对结果有很大影响，先按最简单的情况，方差已知，来讨论。

先考虑检验问题①，记样本均值为$\overline{X}$，$\overline{X}$是$\theta$的估计。故$\overline{X}$越大，与原假设越符合。写成检验的形式如下：
$$
\Phi:当\overline{X}\geq C 时接受原假设H_0，\overline{X}< C 时否定H_0
$$
接下来将进行一系列骚操作，为了防止迷失自我，先回顾一下3.1.3节中的**保一望二原则：先保证第一类错误（原假设正确，但被否定）的概率不超过某指定值$\alpha$（检验水平），再在这限制下，使第二类错误（原假设错误，但被接受）概率尽可能小。**

所以第一步是考虑如何**保一**。先把 “原假设被否定的概率不超过$\alpha$” 写成概率的形式：
$$
P(H_0被否定)\leq \alpha
$$
$H_0$被否定的概率即3.1.2节中定义的功效函数 $\beta_\Phi$

结合刚刚构造好的检验形式，可以将$H_0$被否定的概率记为，
$$
\beta_\Phi(\theta)=P_\theta(\overline{X}<C)
$$
又因为总体$\sim N(\theta,\sigma^2)\rightarrow, \sqrt{n}(\overline{X}-\theta)/\sigma \sim N(0,1)$，有
$$
\beta_\Phi(\theta)=P_\theta(\sqrt{n}(\overline{X}-\theta)/\sigma<\sqrt{n}(C-\theta)/\sigma)
$$

以$\Phi$记$\sqrt{n}(\overline{X}-\theta)/\sigma$的分布函数，有
$$
\beta_\Phi(\theta)=\Phi(\sqrt{n}(C-\theta)/\sigma)\leq \alpha
$$

因为分布函数本身是非降的（这一点要牢记，后面将反复使用），通过上式可知，$\theta$越大，分布函数$\Phi$的值越小。若想要上式在$\theta \geq \theta_0$<font color = blue>(注意，考虑保一时，以原假设正确为前提，即$\theta \geq \theta_0$。)</font>时成立，只要$\beta_\Phi(\theta_0)=\alpha$即可，
$$
\beta_\Phi(\theta_0)=\Phi(\sqrt{n}(C-\theta_0)/\sigma)= \alpha
$$
由上式便可得出$C$，
$$
C=\theta_0-\sigma u_\alpha / \sqrt{n}
$$
推导过程略，需要用到下面几个式子，
$$
\Phi(u_\alpha)=1-\alpha; 
\Phi(u_{1-\alpha})=\alpha;
u_{1-\alpha}=-u_\alpha;
(0<u_\alpha<1, 0<u_{1-\alpha}<1)
$$
将$C$代入$\beta_\Phi(\theta)$，得到功效函数的完成版如下，
$$
\beta_\Phi(\theta)=\Phi(\sqrt{n}(\theta_0-\theta)/\sigma-u_\alpha)
$$
好了，这个功效函数就可以确保“保一望二”里的“保一”了，即原假设正确、但被否定的概率不超过$\alpha$。

-------

建议在这里休息一下，给大脑降降温。

-----



下面让我们看看如何考虑“望二”，即犯第二类错误（原假设错误、但被接受）的概率尽可能小。

<font color = blue>(注意，考虑望二时，以原假设错误为前提，即$\theta < \theta_0$。)</font>

仔细观察刚刚得到的功效函数完成版，重写一遍如下，
$$
\beta_\Phi(\theta)=\Phi(\sqrt{n}(\theta_0-\theta)/\sigma-u_\alpha)
$$
若$\theta < \theta_0$，则可以得出以下几条结论：

(a). $\theta$越小，$\beta_\Phi(\theta)$越大，原假设被否定的概率越大，因为原假设在这里是错的，所以$\beta_\Phi(\theta)$越大越好。可是当$\theta$很接近$\theta_0$时，$\beta_\Phi(\theta)\thickapprox \alpha$，$\alpha$一般是个很小的数，因此此时接受错误的原假设的概率为$1-\beta_\Phi(\theta)\thickapprox 1-\alpha$很接近1，这个是不OK的。

(b). 对固定的$\theta < \theta_0$，$\sigma$越大，$\beta_\Phi(\theta)$越小。也就是说，误差越大，接受错误的原假设的概率越大。

(c). $\alpha$越大，$u_\alpha$越小，$\beta_\Phi(\theta)$越大。也就是说，能容许的第一类错误的概率越大，犯第二类错误的概率越小。<font color = blue>这里可以看出两种错误的矛盾关系。</font>

只通过观察能得出的结论快被榨干了，那要想控制犯第二类错误的概率，该怎么办呢？

设犯第二类错误的概率小于指定值$\beta$（$\beta$是一个大于0的较小的数），即
$$
\beta_\Phi(\theta)\geq 1-\beta, \theta < \theta_0
$$
但是，由上文的结论(a)已知，当$\theta$很接近$\theta_0$时，$\beta_\Phi(\theta)\thickapprox \alpha$，$\alpha$一般是个很小的数，$\beta$也是一个较小的数，两全不能齐美，上式的要求是无法达到的。只能放松一点，要求对某个指定的$\theta_1 < \theta_0$，有
$$
\beta_\Phi(\theta)\geq 1-\beta, \theta < \theta_1
$$
$\theta$越小，$\beta_\Phi(\theta)$越大，因此只需满足
$$
\beta_\Phi(\theta_1)\geq 1-\beta
$$

$$
\beta_\Phi(\theta_1)=\Phi(\sqrt{n}(\theta_0-\theta_1)/\sigma-u_\alpha)\geq 1-\beta
$$

$$
\sqrt{n}(\theta_0-\theta_1)/\sigma-u_\alpha\geq u_\beta
$$

$$
n\geq \sigma^2(u_\alpha+u_\beta)^2/(\theta_0-\theta_1)^2
$$

就是说，样本大小至少应达到上式右边那么大，才能保证$\theta < \theta_1$时，犯第二类错误的概率小于$\beta$。

神奇不，最后竟然推出来了一个样本大小的下限值。

式中的$n$与$\sigma^2$成正比，即方差越大，为达到一定的分辨率，所需要的样本数也越多。

好了，现在是不是明白了为什么要分两类错误，为什么错误一和错误二不能兼顾，以及“保一望二”是如何操作的了？回答不出来的话，回去重看一遍。

简单点讲，之所以有以上种种麻烦，就是因为我们在检验问题里设定的临界值C，和假设里的$\theta_0$不是完全相等的，要不偏左一点点，要不偏右一点点，在这个包含了$\theta_0$的有一定宽度的分界处，估计值落在哪里的概率都差不多，落在$\theta_0$某一侧的概率很接近落在$\theta_0$另一侧的概率，想要一侧很小而另一侧很大是不可能的。

---

缓一下，接下来分析检验问题②（我为神魔要整理这些。。。有点后悔）

---

检验问题 ②　$H_0': \theta \leq \theta_0, H_1': \theta > \theta_0$

$\overline{X}$越小，与原假设越符合。写成检验的形式如下：
$$
\Phi:当\overline{X}\leq \theta_0+\sigma u_\alpha / \sqrt{n} 时接受H_0'，不然就否定H_0'
$$
功效函数为：





###### 3.2.2 两个正态总体均值差的检验

###### 3.2.3 正态分布方差的检验

###### 3.2.4 指数分布参数的检验

###### 3.2.5 二项分布参数$p$的检验

###### 3.2.6 泊松分布参数$\lambda$的检验

###### 3.2.7 大样本检验

###### 3.2.9 贝叶斯方法



##### 3.3 拟合优度检验

###### 3.3.1 理论分布完全已知且只取有限个值的情况

###### 3.3.2 理论分布只含有限个值但不完全已知的情况

###### 3.3.3 对列联表的应用

###### 3.3.4 总体分布为一般分布的情形



##### 3.4 基本步骤（见DataWhale参考资料）



##### 3.5 统计量的选择

选择合适的统计量是进行假设检验的关键步骤，最常用的统计检验包括回归检验(regression test)，比较检验(comparison test)和关联检验(correlation test)三类。

**回归检验**

回归检验适用于预测变量是数值型的情况，根据预测变量的数量和结果变量的类型又分为以下几种。

|              |    预测变量    | 结果变量 |
| :----------: | :------------: | :------: |
| 简单线性回归 | 单个，连续数值 | 连续数值 |
| 多重线性回归 | 多个，连续数值 | 连续数值 |
| Logistic回归 |    连续数值    | 二元类别 |

**比较检验**

比较检验适用于预测变量是类别型，结果变量是数值型的情况，根据预测变量的分组数量和结果变量的数量又可以分为以下几种。

|                    |     预测变量     |       结果变量       |
| :----------------: | :--------------: | :------------------: |
|   Paired t-test    |    两组，类别    | 组来自同一总体，数值 |
| Independent t-test |    两组，类别    | 组来自不同总体，数值 |
|       ANOVA        | 两组及以上，类别 |      单个，数值      |
|       MANOVA       | 两组及以上，类别 |   两个及以上，数值   |

**关联检验**

关联检验常用的只有卡方检验一种，适用于预测变量和结果变量均为类别型的情况。

**非参数检验**

此外，由于一般来说上述参数检验都需满足一些前提条件，样本之间独立，不同组的组内方差近似和数据满足正态性，所以当这些条件不满足的时候，我们可以尝试用非参数检验来代替参数检验。

|        非参数检验         | 用于替代的参数检验 |
| :-----------------------: | :----------------: |
|         Spearman          |   回归和关联检验   |
|         Sign test         |       T-test       |
|      Kruskal–Wallis       |       ANOVA        |
|          ANOSIM           |       MANOVA       |
|  Wilcoxon Rank-Sum test   | Independent t-test |
| Wilcoxon Signed-rank test |   Paired t-test    |



##### 3.6 Python代码实战（见DataWhale参考资料）

* 正态检验

* 卡方检验

* T-test

* ANOVA

* Mann-Whitney U test